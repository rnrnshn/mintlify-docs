---
title: "A.I Agents"
description: "Bem-vindo ao guia definitivo para configurar um ambiente local para criar e usar agentes de IA com Ollama e LangChain no Windows e MacOS!"
---

Este tutorial passo a passo vai te guiar para montar um setup robusto, perfeito para rodar modelos de linguagem localmente e construir pipelines **RAG (Retrieval-Augmented Generation)** com **SingleStore** ou **ChromaDB**. Tudo isso com privacidade total e sem custos de API\!

## Guia de ConfiguraÃ§Ã£o: AI Agents com Ollama e LangChain

### **Objetivo**

Configurar um ambiente funcional para desenvolver e testar agentes de IA localmente, com suporte para modelos de linguagem (LLMs) e armazenamento vetorial.   Ideal para iniciantes e experts\!

### PrÃ©-requisitos

**Sistema Operacional**

- Windows 11/10 ou MacOS (Ventura ou posterior recomendado).

**Hardware**

- MÃ­nimo: 8 GB de RAM
- Recomendado: 16 GB de RAM
- EspaÃ§o livre: 10 GB

**Software**

- Python 3.9\+ instalado.
- Docker Desktop instalado e em execuÃ§Ã£o (para SingleStore, se desejar).
- Terminal: PowerShell (Windows) ou Terminal (MacOS).
- ConexÃ£o Ã  internet para baixar modelos e dependÃªncias (apÃ³s configuraÃ§Ã£o, tudo roda offline\!).

## **Passo a Passo**

### Passo 1: Instalar o Ollama

O Ollama permite rodar modelos de linguagem localmente com facilidade.

**Download e InstalaÃ§Ã£o**

**MacOS**

```shellscript
curl -fsSL https://ollama.ai/install.sh | sh
```

[******Ou baixe o instalador em ollama.com e siga as instruÃ§Ãµes.******](http://ollama.com)

**Verificar instalaÃ§Ã£o**

```shellscript
ollama --version
```

**Deve retornar 0.1.x ou superior**

**Baixar Modelos**

```shellscript
ollama serve
ollama pull llama3.2
ollama pull mxbai-embed-large
ollama list
```

### Passo 2: Configurar o Ambiente Python 

Um ambiente virtual mantÃ©m tudo organizado e isolado.

**Verificar Python**

**MacOS**

```shellscript
python3 --version
```

**Se nÃ£o instalado:**

```shellscript
brew install python
```

**Windows**

```
python --version
```

[******Se nÃ£o instalado, baixe de python.org.******](http://python.org)

Criar diretÃ³rio do projeto mkdir ai-agent-project cd ai-agent-project

Criar e ativar ambiente virtual

**MacOS**

```shellscript
python3 -m venv venv source venv/bin/activate
```

**Windows**

```shellscript
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**Instalar dependÃªncias**

```shellscript
pip install langchain langchain-ollama langchain-singlestore pandas
```

**Opcional (usando Poetry)**

```shellscript
pip install poetry
poetry new ai_tutorial
cd ai_tutorial
poetry add langchain langchain-ollama langchain-singlestore pandas
```

### Passo 3: Configurar o SingleStore (Opcional para RAG)

**Instalar Docker**

**MacOS**

```shellscript
brew install --cask docker
```

[******Windows Baixe em docker.com.******](http://docker.com)

Inicie o Docker Desktop

**O script no Passo 4 jÃ¡ inicializa o SingleStore automaticamente.**

### Passo 4: Criar o Agente de IA

Criar arquivo de dados (opcional)

```shellscript
pizza_reviews.csv
```

**Title,Date,Rating,Review "Best Pizza in Town\!","2025-01-06",1,"The crust was perfectly crispy and the toppings were fresh." "Delicious and Affordable","2024-08-12",1,"A bit overpriced, but overall a decent meal."**

**Criar script Python**

```shellscript
main.py
```

```python
import pandas as pd
from singlestoredb.server import docker
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_singlestore import SingleStoreVectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.documents import Document
â€‹
def setup_database(s2db):
 Â  Â """Inicializa o banco de dados SingleStore."""
 Â  Â with s2db.connect() as conn:
 Â  Â  Â  Â with conn.cursor() as cursor:
 Â  Â  Â  Â  Â  Â cursor.execute("CREATE DATABASE IF NOT EXISTS testdb")
â€‹
def load_documents():
 Â  Â """Carrega revisÃµes de pizzas do CSV."""
 Â  Â df = pd.read_csv("pizza_reviews.csv")
 Â  Â documents = []
 Â  Â for i, row in df.iterrows():
 Â  Â  Â  Â content = f"{row['Title']} {row['Review']}"
 Â  Â  Â  Â documents.append(
 Â  Â  Â  Â  Â  Â Document(
 Â  Â  Â  Â  Â  Â  Â  Â page_content=content,
 Â  Â  Â  Â  Â  Â  Â  Â metadata={"rating": row["Rating"], "date": row["Date"]},
 Â  Â  Â  Â  Â  Â  Â  Â id=str(i)
 Â  Â  Â  Â  Â   )
 Â  Â  Â   )
 Â  Â return documents
â€‹
def main():
 Â  Â print("ğŸš€ Iniciando SingleStoreDB para armazenamento vetorial...")
 Â  Â with docker.start(license="") as s2db:
 Â  Â  Â  Â setup_database(s2db)
 Â  Â  Â  Â print("ğŸ“‚ Carregando e criando embeddings das revisÃµes de pizzas...")
 Â  Â  Â  Â documents = load_documents()
 Â  Â  Â  Â embedding = OllamaEmbeddings(model="mxbai-embed-large")
 Â  Â  Â  Â vector_store = SingleStoreVectorStore(
 Â  Â  Â  Â  Â  Â embedding=embedding,
 Â  Â  Â  Â  Â  Â host=s2db.connection_url,
 Â  Â  Â  Â  Â  Â database="testdb",
 Â  Â  Â  Â  Â  Â table_name="pizza_reviews",
 Â  Â  Â   )
 Â  Â  Â  Â vector_store.add_documents(documents)
 Â  Â  Â  Â retriever = vector_store.as_retriever(search_kwargs={"k": 2})
 Â  Â  Â  Â print("ğŸ§  Inicializando modelo LLaMA 3.2...")
 Â  Â  Â  Â model = OllamaLLM(model="llama3.2")
 Â  Â  Â  Â template = """
 Â  Â  Â   VocÃª Ã© um especialista em responder perguntas sobre um restaurante de pizzas. ğŸ•
 Â  Â  Â   RevisÃµes relevantes: {reviews}
 Â  Â  Â   Pergunta: {question}
 Â  Â  Â   """
 Â  Â  Â  Â prompt = ChatPromptTemplate.from_template(template)
 Â  Â  Â  Â chain = prompt | model
 Â  Â  Â  Â print("\n--- Sistema de Perguntas e Respostas sobre RevisÃµes de Pizzas ---")
 Â  Â  Â  Â while True:
 Â  Â  Â  Â  Â  Â user_input = input("\nDigite sua pergunta sobre pizzas (ou 'exit' para sair): ")
 Â  Â  Â  Â  Â  Â if user_input.lower() == "exit":
 Â  Â  Â  Â  Â  Â  Â  Â break
 Â  Â  Â  Â  Â  Â print("\nğŸ” Buscando revisÃµes relevantes e gerando resposta...")
 Â  Â  Â  Â  Â  Â reviews = retriever.invoke(user_input)
 Â  Â  Â  Â  Â  Â result = chain.invoke({"reviews": reviews, "question": user_input})
 Â  Â  Â  Â  Â  Â print("\n--- Resposta ---")
 Â  Â  Â  Â  Â  Â print(result)
â€‹
if __name__ == "__main__":
 Â  Â main()
â€‹
```

**Executar**

**Certifique-se de que o Ollama estÃ¡ rodando:**

```shellscript
ollama serve
```

**Depois:**

```shellscript
python main.py
```

**Ou com Poetry:**

```shellscript
poetry run python main.py
```

### Passo 5: Testar e Personalizar

Testar exemplo

Digite: **O que as pessoas acham da crosta da pizza?**

Personalizar

Trocar modelo (llama3.2 â†’ mistral, qwen2:7b etc.).

Adicionar mais dados ao pizza_reviews.csv.

Integrar ferramentas externas (acesso web, arquivos, etc.).

### Passo 6: SoluÃ§Ã£o de Problemas

**Ollama nÃ£o conecta**

```shellscript
ollama serve
sudo lsof -i :11434 # Mac
netstat -aon | findstr :11434 # Windows
```

**Modelo nÃ£o encontrado**

```shellscript
ollama list
ollama pull <modelo>
```

**Erro de mÃ³dulo Python**

```shellscript
pip install -U langchain langchain-ollama langchain-singlestore pandas
```

### Passo 7: PrÃ³ximos Passos

Adicionar interface web com Streamlit

Testar com ChromaDB

Implementar multi-agentes

Adicionar memÃ³ria de conversaÃ§Ã£o

## Notas Finais

100% Local: Privacidade garantida, sem APIs externas. 

<Card title="Recursos Ãšteis DocumentaÃ§Ã£o Ollama" icon="link">
  LangChain Docs

  SingleStore Tutorials

  DocumentaÃ§Ã£o do Ollama

  DocumentaÃ§Ã£o do LangChain

  Tutoriais da SingleStore

  Agora vocÃª estÃ¡ pronto para construir agentes de IA incrÃ­veis\! ğŸ‰
</Card>